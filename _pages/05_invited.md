---
layout: page
permalink: /talks/
title: Talks
description:
---

#### Invited talks

#### [08:00-08:30am EST](https://www.starts-at.com/event/2835043211): [Max Welling](https://staff.fnwi.uva.nl/m.welling/), University of Amsterdam

**The LIAR (Learning with Interval Arithmetic Regularization) is Dead**

2 years ago we embarked on a project called LIAR. LIAR was going to quantify uncertainty of a network through interval arithmetic (IA) calculations (which are an official IEEE standard).  IA has the beautiful property that the answer of your computation is guaranteed to lie in a computed interval, and as such quantifies very precisely the numerical precision of your computation.  Captured by this elegant idea we applied this to neural networks. In particular, the idea was to add a regularization term to the objective that would try to keep the interval of the network's output small. This is particularly interesting in the context of quantization, where we quite naturally have intervals for the weights, activations and inputs due to their limited precision. By training a full precision neural network with intervals that represent the quantization error, and by encouraging the network to keep the resultant variation in the predictions small, we hoped to learn networks that were inherently robust to quantization noise. So far the good news. In this talk I will try to reconstruct the process of how the project ended up on the scrap pile. I will also try to produce some "lessons learned" from this project and hopefully deliver some advice for those who are going through a similar situation. I still can't believe it didn't work better ;-)



#### 08:30-09:00am EST: [Danielle Belgrave](https://www.microsoft.com/en-us/research/people/dabelgra/), Microsoft Research

**Machine Learning for Personalised Healthcare: Why is it not better?**

This talk presents an overview of probabilistic graphical modelling as a strategy for understanding heterogeneous subgroups of patients. The identification of such subgroups may elucidate underlying causal mechanisms which may lead to more targeted treatment and intervention strategies. We will look at (1) the ideal of personalisation within the context of machine learning for healthcare (2) “From the ideal to the reality” and (3) some of the possible pathways to progress for making the ideal of personalised healthcare to reality. The last part of this talk focuses on the pipeline of personalisation and looks at probabilistic graphical models are part of a pipeline.


#### 09:00-09:30am EST: [Michael C. Hughes](https://www.michaelchughes.com/), Tufts University

**The Case for Prediction Constrained Training**

This talk considers adding supervision to well-known generative latent variable models (LVMs), including both classic LVMs (e.g. mixture models, topic models) and more recent “deep” flavors (e.g. variational autoencoders). The standard way to add supervision to LVMs would be to treat the added label as another observed variable generated by the graphical model, and then maximize the joint likelihood of both labels and features. We find that across many models, this standard supervision leads to surprisingly negligible improvement in prediction quality over a more naive baseline that first fits an unsupervised model, and then makes predictions given that model's learned low-dimensional representation. We can’t believe it is not better! Further, this problem is not properly solved by previous approaches that just upweight or “replicate” labels in the generative model (the problem is not just that we have more observed features than labels). Instead, we suggest the problem is related to model misspecification, and that the joint likelihood objective does not properly encode the desired performance goals at test time (we care about predicting labels from features, but not features from labels). This motivates a new training objective we call prediction constrained training, which can prioritize the label-from-feature prediction task while still delivering reasonable generative models for the observed features. We highlight promising results of our proposed prediction-constrained framework including recent extensions to semi-supervised VAEs and model-based reinforcement learning.

#### 1:00-1:30pm EST: [Andrew Gelman](http://www.stat.columbia.edu/~gelman/), Columbia University

**It Doesn't Work, But The Alternative Is Even Worse:  Living With Approximate Computation**

We can't fit the models we want to fit because it takes too long to fit them on our computer.  Also, we don't know what models we want to fit until we try a few.  I share some stories of struggles with data-partitioning and parameter-partitioning algorithms, what kinda worked and what didn't.


#### 1:30-2:00pm EST: Roger Grosse, University of Toronto

**Why Isn’t Everyone Using Second-Order Optimization?**

In the pre-AlexNet days of deep learning, second-order optimization gave dramatic speedups and enabled training of deep architectures that seemed to be inaccessible to first-order optimization. But today, despite algorithmic advances such as K-FAC, nearly all modern neural net architectures are trained with variants of SGD and Adam. What’s holding us back from using second-order optimization?  I’ll discuss three challenges to applying second-order optimization to modern neural nets: difficulty of implementation, implicit regularization effects of gradient descent, and the effect of gradient noise. All of these factors are significant, though not in the ways commonly believed.

#### 2:00-2:30pm EST: Weiwei Pan, Harvard University

**What are Useful Uncertainties for Deep Learning and How Do We Get Them?**

While deep learning has demonstrable success on many tasks, the point estimates provided by standard deep models can lead to overfitting and provide no uncertainty quantification on predictions.  However, when models are applied to critical domains such as autonomous driving, precision health care, or criminal justice, reliable measurements of a model's predictive uncertainty may be as crucial as correctness of its predictions. In this talk, we examine a number of deep (Bayesian) models that promise to capture complex forms for predictive uncertainties, we also examine metrics commonly used to such uncertainties. We aim to highlight strengths and limitations of these models as well as the metrics; we also discuss ideas to improve both in meaningful ways for downstream tasks.


#### Contributed Talks
