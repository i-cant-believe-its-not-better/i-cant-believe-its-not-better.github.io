---
layout: page
permalink: /accepted_papers/
title: Accepted
description: ICBINB@NeurIPS 2020
---

* Erik Jones, Shiori Sagawa, Pang Wei Koh, Ananya Kumar, Percy Liang. <b>Selective Classification Can Magnify Disparities Across Groups</b>
<!-- * Erik Jones, Shiori Sagawa, Pang Wei Koh, Ananya Kumar, Percy Liang. <b>Selective Classification Can Magnify Disparities Across Groups</b> [<a href="">link</a>] -->

* Ricky T. Q. Chen, Dami Choi, Lukas Balles, David Duvenaud, Philipp Hennig. <b>Self-Tuning Stochastic Optimization with Curvature-Aware Gradient Filtering</b>

* Udari Madhushani, Naomi Leonard. <b>It Doesn’t Get Better and Here’s Why: A Fundamental Drawback in Natural Extensions of UCB to Multi-agent Bandits</b>

* Tin D. Nguyen, Jonathan H. Huggins, Lorenzo Masoero, Lester Mackey, Tamara Broderick. <b>Independent versus truncated finite approximations for Bayesian nonparametric inference</b>

* Charline Le Lan, Laurent Dinh. <b>Perfect density models cannot guarantee anomaly detection</b>

* Elliott Gordon-Rodriguez, Gabriel Loaiza-Ganem, Geoff Pleiss, John Patrick Cunningham. <b>Uses and Abuses of the Cross-Entropy Loss: Case Studies in Modern Deep Learning</b>

* Ângelo Gregório Lovatto, Thiago Pereira Bueno, Denis Mauá, Leliane Nunes de Barros. <b>Decision-Aware Model Learning for Actor-Critic Methods: When Theory Does Not Meet Practice</b>

* Maurice Frank, Maximilian Ilse. <b>Problems using deep generative models for probabilistic audio source separation</b>

* W Ronny Huang, Zeyad Ali Sami Emam, Micah Goldblum, Liam H Fowl, Justin K Terry, Furong Huang, Tom Goldstein. <b>Understanding Generalization through Visualizations</b>

* Yannick Rudolph, Ulf Brefeld, Uwe Dick. <b>Graph Conditional Variational Models: Too Complex for Multiagent Trajectories?</b>

* Emilio Jorge, Hannes Eriksson, Christos Dimitrakakis, Debabrota Basu, Divya Grover. <b>Inferential Induction: A Novel Framework for Bayesian Reinforcement Learning</b>

* Fan Bao, Kun Xu, Chongxuan Li, Lanqing HONG, Jun Zhu, Bo Zhang. <b>Variational (Gradient) Estimate of the Score Function in Energy-based Latent Variable Models</b>

* Ziyu Wang, Bin Dai, David Wipf, Jun Zhu. <b>Further Analysis of Outlier Detection with Deep Generative Models</b>

* Ramiro Camino, Chris Hammerschmidt, Radu State. <b>Oversampling Tabular Data with Deep Generative Models: Is it worth the effort?</b>

* Siwen Yan, Devendra Singh Dhami, Sriraam Natarajan. <b>The Curious Case of Stacking Boosted Relational Dependency Networks</b>

* Vincent Fortuin, Adrià Garriga-Alonso, Florian Wenzel, Gunnar Ratsch, Richard E Turner, Mark van der Wilk, Laurence Aitchison. <b>Bayesian Neural Network Priors Revisited</b>

* Stella Biderman, Walter Scheirer. <b>Pitfalls in Machine Learning Research: Reexamining the Development Cycle</b>

* Mihaela Rosca, Theophane Weber, Arthur Gretton, Shakir Mohamed. <b>A case for new neural networks smoothness constraints</b>

* Seungjae Jung, Kyung-Min Kim, Hanock Kwak, Young-Jin Park. <b>A Worrying Analysis of Probabilistic Time-series Models for Sales Forecasting</b>

* Diana Cai, Trevor Campbell, Tamara Broderick. <b>Power posteriors do not reliably learn the number of components in a finite mixture</b>

* Sachin Kumar, Yulia Tsvetkov. <b>End-to-End Differentiable GANs for Text Generation</b>

* Jovana Mitrovic, Brian McWilliams, Melanie Rey. <b>Less can be more in contrastive learning</b>

* Ilya Kavalerov, Wojciech Czaja, Rama Chellappa. <b>A study of quality and diversity in K+1 GANs</b>

* Saiteja Utpala, Piyush Rai. <b>Temperature Scaling for Quantile Calibration</b>

* Margot Selosse, Claire Gormley, Julien Jacques, Christophe Biernacki. <b>A bumpy journey: exploring deep Gaussian mixture models</b>

* Haydn Thomas Jones, Juston Moore. <b>Is the Discrete VAE’s Power Stuck in its Prior?</b>

* Jeremy Nixon, Balaji Lakshminarayanan, Dustin Tran. <b>Why Are Bootstrapped Deep Ensembles Not Better?</b>

* Matthias Rosynski, Frank Kirchner, Matias Valdenegro-Toro. <b>Are Gradient-based Saliency Maps Useful in Deep Reinforcement Learning?</b>

* Bo Pang, Erik Nijkamp, Jiali Cui, Tian Han, Ying Nian Wu. <b>Semi-supervised Learning by Latent Space Energy-Based Model of Symbol-Vector Coupling</b>

* Thibault Lesieur, Jérémie Messud, Issa Hammoud, Hanyuan Peng, Céline Lacombe, Paulien Jeunesse. <b>Adversarial training for predictive tasks: theoretical analysis and limitations in the deterministic case.</b>

* Kai-Chun Hu, Ping-Chun Hsieh, Ting Han Wei, I-Chen Wu. <b>Rethinking Deep Policy Gradients via State-Wise Policy Improvement</b>

* Akshatha Kamath, Dwaraknath Gnaneshwar, Matias Valdenegro-Toro. <b>Know Where To Drop Your Weights: Towards Faster Uncertainty Estimation</b>

* Hoang Thanh-Tung, Truyen Tran. <b>Toward a Generalization Metric for Deep Generative Models</b>

* Joseph Turian, Max Henry. <b>I’m Sorry for Your Loss: Spectrally-Based Audio Distances Are Bad at Pitch</b>
